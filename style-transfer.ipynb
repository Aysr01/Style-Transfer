{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1 style=\"color:orange;font-weight:900;font-size:50px;\"> Styel Transfer From Scratch </h1></center> <br>\n<img src=\"https://media.licdn.com/dms/image/C4E12AQEfjA-SVxYLVQ/article-cover_image-shrink_600_2000/0/1531630356496?e=2147483647&v=beta&t=kmO2CHjqruhnAASb4Ejpu5-GKwe-7L7HjYbwZD2N4oY\">","metadata":{"id":"gcXfsQoXTf_6"}},{"cell_type":"markdown","source":"<h3>What is style transfer ? </h3>\n\n<p style=\"font-size:17px\">Style transfer is a technique in computer vision and deep learning that involves applying the artistic style of one image to the content of another. It combines the content and texture features of two images to create a new image that retains the content of one while adopting the style characteristics of the other. This process is often used to create visually appealing and artistic images or videos, merging the content of a photograph with the artistic style of famous paintings, for example.</p>","metadata":{}},{"cell_type":"markdown","source":"<h3>Objectives of this note book</h3>\n<p style=\"font-size:17px;color:black\">In this notebook, our objective is to develop a style transfer model using the VGG19 pretrained model.<p>","metadata":{}},{"cell_type":"markdown","source":"## Importing packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport PIL\nimport IPython.display as display\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nprint(tf.__version__)","metadata":{"id":"hz9rOk_KTeWf","execution":{"iopub.status.busy":"2023-09-01T13:02:25.126098Z","iopub.status.idle":"2023-09-01T13:02:25.126860Z","shell.execute_reply.started":"2023-09-01T13:02:25.126616Z","shell.execute_reply":"2023-09-01T13:02:25.126641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading VGG19 model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.applications.vgg19.VGG19(include_top = False,\n                                    weights=\"imagenet\")\nmodel.trainable = False\n","metadata":{"id":"NKF7dlD5fmLJ","outputId":"9660412a-2232-4909-a40e-631719602838","execution":{"iopub.status.busy":"2023-09-01T12:53:08.239150Z","iopub.execute_input":"2023-09-01T12:53:08.240317Z","iopub.status.idle":"2023-09-01T12:53:13.401324Z","shell.execute_reply.started":"2023-09-01T12:53:08.240278Z","shell.execute_reply":"2023-09-01T12:53:13.400258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://camo.githubusercontent.com/58a6d61b5f1ca839636a79055a076753683df49a5d81b21f009c44fc345b840c/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313239342f312a5a6757353230535a7231516b476f4664337871594d772e6a706567\">","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 17px\">Beginning with the network's input layer, the initial layers capture basic elements such as edges and textures. Progressing deeper into the network, the later layers capture more complex features like specific object components such as wheels or eyes. In this instance, we are employing the VGG19 network architecture, which has been pretrained for image classification. These intermediate layers play a crucial role in defining how content and style are represented in the images.</p>","metadata":{}},{"cell_type":"code","source":"content_layers = ['block5_conv2']\n\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1',\n                'block4_conv1',\n                'block5_conv1']\n\nnumber_style_layers = len(style_layers)","metadata":{"id":"D4NrszhqgZWO","execution":{"iopub.status.busy":"2023-09-01T12:53:13.402565Z","iopub.execute_input":"2023-09-01T12:53:13.402915Z","iopub.status.idle":"2023-09-01T12:53:13.407784Z","shell.execute_reply.started":"2023-09-01T12:53:13.402882Z","shell.execute_reply":"2023-09-01T12:53:13.406879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting the output of content layers\ncontent_outputs = [model.get_layer(name).output for name in content_layers]\n\n#getting the output of style layers\nstyle_outputs = [model.get_layer(name).output for name in style_layers]\n\n# building the model\ntransfer_model = tf.keras.models.Model(inputs = model.inputs,\n                                      outputs = style_outputs + content_outputs)\n\ntransfer_model.trainable = False","metadata":{"id":"eLrSZIi-hHUB","execution":{"iopub.status.busy":"2023-09-01T12:53:13.410975Z","iopub.execute_input":"2023-09-01T12:53:13.411652Z","iopub.status.idle":"2023-09-01T12:53:13.431047Z","shell.execute_reply.started":"2023-09-01T12:53:13.411619Z","shell.execute_reply":"2023-09-01T12:53:13.430159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transfer_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:53:13.432490Z","iopub.execute_input":"2023-09-01T12:53:13.432836Z","iopub.status.idle":"2023-09-01T12:53:13.477904Z","shell.execute_reply.started":"2023-09-01T12:53:13.432803Z","shell.execute_reply":"2023-09-01T12:53:13.477225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Preprocessing","metadata":{}},{"cell_type":"code","source":"def load_image(img_path):\n   max_size = 512\n   image = PIL.Image.open(img_path)\n   image_shape = tf.shape(image).numpy()\n   if (y:=max(image_shape)) > 512:\n        scale = max_size / y \n        new_shape = tf.cast(image_shape[:-1] * scale, tf.int32)\n        image = tf.image.resize(image, new_shape)\n   image = tf.expand_dims(image , axis=0)\n   return image.numpy().astype(\"uint8\")","metadata":{"id":"Q--QfhMYrdh9","execution":{"iopub.status.busy":"2023-09-01T12:53:13.478821Z","iopub.execute_input":"2023-09-01T12:53:13.479188Z","iopub.status.idle":"2023-09-01T12:53:13.494466Z","shell.execute_reply.started":"2023-09-01T12:53:13.479143Z","shell.execute_reply":"2023-09-01T12:53:13.493670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show(image, title):\n\n  plt.imshow(image)\n  plt.axis(\"off\")\n  plt.title(title)","metadata":{"id":"11l7pxEnUmdT","execution":{"iopub.status.busy":"2023-09-01T12:53:13.495509Z","iopub.execute_input":"2023-09-01T12:53:13.495849Z","iopub.status.idle":"2023-09-01T12:53:13.504721Z","shell.execute_reply.started":"2023-09-01T12:53:13.495818Z","shell.execute_reply":"2023-09-01T12:53:13.503891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:17px\">In the training process of VGG19, the images was converted from the RGB color space to the BGR color space. Additionally, each color channel in the BGR images is zero-centered with respect to the ImageNet dataset, without scaling.</p>","metadata":{}},{"cell_type":"code","source":"def img_preprocess(img_path):\n    image=load_image(img_path)\n    # the input of vgg19 should be necessary a numpy array\n    image=tf.keras.applications.vgg19.preprocess_input(image)\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:53:13.505888Z","iopub.execute_input":"2023-09-01T12:53:13.506288Z","iopub.status.idle":"2023-09-01T12:53:13.511759Z","shell.execute_reply.started":"2023-09-01T12:53:13.506256Z","shell.execute_reply":"2023-09-01T12:53:13.510970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deprocess_img(processed_img):\n  x = processed_img.copy()\n  if len(x.shape) == 4:\n    x = np.squeeze(x, 0)\n  assert len(x.shape) == 3 #Input dimension must be [1, height, width, channel] or [height, width, channel]\n  \n  \n  # perform the inverse of the preprocessing step\n  x[:, :, 0] += 103.939\n  x[:, :, 1] += 116.779\n  x[:, :, 2] += 123.68\n  x = x[:, :, ::-1] # converting BGR to RGB channel\n\n  x = np.clip(x, 0, 255).astype('uint8')\n  return x","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:53:13.513064Z","iopub.execute_input":"2023-09-01T12:53:13.513445Z","iopub.status.idle":"2023-09-01T12:53:13.522247Z","shell.execute_reply.started":"2023-09-01T12:53:13.513413Z","shell.execute_reply":"2023-09-01T12:53:13.521264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downlowding images from Internet","metadata":{}},{"cell_type":"code","source":"!wget https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg\n!wget https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:53:13.528352Z","iopub.execute_input":"2023-09-01T12:53:13.529134Z","iopub.status.idle":"2023-09-01T12:53:15.739398Z","shell.execute_reply.started":"2023-09-01T12:53:13.529100Z","shell.execute_reply":"2023-09-01T12:53:15.738259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_image = load_image(\"YellowLabradorLooking_new.jpg\")\nstyle_image = load_image(\"Vassily_Kandinsky,_1913_-_Composition_7.jpg\")","metadata":{"id":"uwnXexKJWqab","outputId":"3bbb3c37-7e6a-42ab-c383-980f16314e63","execution":{"iopub.status.busy":"2023-09-01T12:53:15.741144Z","iopub.execute_input":"2023-09-01T12:53:15.741840Z","iopub.status.idle":"2023-09-01T12:53:15.811082Z","shell.execute_reply.started":"2023-09-01T12:53:15.741800Z","shell.execute_reply":"2023-09-01T12:53:15.810072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(121)\nshow(content_image[0], \"Content image\")\n\nplt.subplot(122)\nshow(style_image[0] , \"Style image\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:53:15.812468Z","iopub.execute_input":"2023-09-01T12:53:15.812825Z","iopub.status.idle":"2023-09-01T12:53:16.124221Z","shell.execute_reply.started":"2023-09-01T12:53:15.812790Z","shell.execute_reply":"2023-09-01T12:53:16.123300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss function","metadata":{}},{"cell_type":"markdown","source":"### ${Content Loss} = \\frac{1}{2} \\sum (F(G) - F(C))^2$\n\n**F(G)(resp F(c)):** represents the feature maps of the generated image (resp content image). It's the output of a specific layer in VGG19 model","metadata":{}},{"cell_type":"code","source":"def content_loss(Cl , Gl):\n  n = tf.cast(len(Cl),tf.float32)\n  return tf.add_n([tf.reduce_mean(tf.square(u - v)) for u,v in zip(Cl,Gl)]) / n","metadata":{"id":"1XaWePRIVKGo","execution":{"iopub.status.busy":"2023-09-01T12:53:16.125181Z","iopub.execute_input":"2023-09-01T12:53:16.125509Z","iopub.status.idle":"2023-09-01T12:53:16.132050Z","shell.execute_reply.started":"2023-09-01T12:53:16.125475Z","shell.execute_reply":"2023-09-01T12:53:16.131062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gram matrix:  \n# $G^l_{cd} = \\frac{\\sum_{ij} F^l_{ijc}(x)F^l_{ijd}(x)}{IJ}$\n`l`: Represents the layer in the neural network where the feature maps are extracted.\n\n`c` and `d`: Represent different channels (feature channels) in the feature maps.\n\n","metadata":{}},{"cell_type":"code","source":"def gram_matrix(input_tensor):\n  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n  input_shape = tf.shape(input_tensor)\n  num_coef = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n  return result/(num_coef)","metadata":{"id":"cm9y8S4GjCfh","execution":{"iopub.status.busy":"2023-09-01T12:53:16.133888Z","iopub.execute_input":"2023-09-01T12:53:16.134679Z","iopub.status.idle":"2023-09-01T12:53:16.142284Z","shell.execute_reply.started":"2023-09-01T12:53:16.134645Z","shell.execute_reply":"2023-09-01T12:53:16.141262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $\\text{Style Loss} = \\sum_{l} (G^l(G) - G^l(S))^2$\n\n**G(x):** represents the gram matrix obtained from the output of a specific layer in VGG19 model when applied to image x","metadata":{}},{"cell_type":"code","source":"def style_loss(Sl , Gl):\n  l = []\n  for u , v in zip(Sl , Gl):\n    gen_sm = gram_matrix(u)\n    style_sm = gram_matrix(v)\n    l.append(tf.reduce_mean(tf.square(style_sm - gen_sm)))\n  return tf.add_n(l) / tf.cast(len(Sl) , tf.float32)","metadata":{"id":"btcQhACGrGIO","execution":{"iopub.status.busy":"2023-09-01T12:53:16.143881Z","iopub.execute_input":"2023-09-01T12:53:16.144225Z","iopub.status.idle":"2023-09-01T12:53:16.152185Z","shell.execute_reply.started":"2023-09-01T12:53:16.144193Z","shell.execute_reply":"2023-09-01T12:53:16.151076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the function return a tuple whose the first element is the output os style layers and\n# the second element is the output os content layers\n\ndef get_features(model, tensor):\n  output = model(tensor)\n  return {\"style\" : output[:number_style_layers],\n          \"content\" : output[number_style_layers:]}","metadata":{"id":"H6MnPJ0uCtLK","execution":{"iopub.status.busy":"2023-09-01T12:53:16.155267Z","iopub.execute_input":"2023-09-01T12:53:16.155575Z","iopub.status.idle":"2023-09-01T12:53:16.163573Z","shell.execute_reply.started":"2023-09-01T12:53:16.155536Z","shell.execute_reply":"2023-09-01T12:53:16.162608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $\\text{Total Loss} = \\alpha \\cdot \\text{Content Loss} + \\beta \\cdot \\text{Style Loss}$","metadata":{}},{"cell_type":"code","source":"def total_loss(style_loss , content_loss , style_weight = 1e-2 , content_weight = 1e4):\n    return style_loss * style_weight + content_loss * content_weight","metadata":{"id":"Qjz4S8DPv5W6","execution":{"iopub.status.busy":"2023-09-01T12:53:16.164962Z","iopub.execute_input":"2023-09-01T12:53:16.165375Z","iopub.status.idle":"2023-09-01T12:53:16.174283Z","shell.execute_reply.started":"2023-09-01T12:53:16.165342Z","shell.execute_reply":"2023-09-01T12:53:16.173437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# avoid explosif grandient\ndef clip(image, min_val, max_val):\n  return tf.clip_by_value(image, clip_value_min=min_val, clip_value_max=max_val)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:53:16.177412Z","iopub.execute_input":"2023-09-01T12:53:16.177734Z","iopub.status.idle":"2023-09-01T12:53:16.190961Z","shell.execute_reply.started":"2023-09-01T12:53:16.177709Z","shell.execute_reply":"2023-09-01T12:53:16.190241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Style Transfer","metadata":{}},{"cell_type":"code","source":"class StyleTransfer:\n\n  @tf.function()\n  def gradient_descent(self, model,optimizer, gen_image, style_features,\n                      content_features, style_weight, content_weight):\n\n      with tf.GradientTape() as g:\n        g.watch(gen_image)\n        gen_image_sfeatures , gen_image_cfeatures = get_features(model, gen_image).values()\n        j_content = content_loss(content_features , gen_image_cfeatures)\n        j_style = style_loss(style_features , gen_image_sfeatures)\n        j_total = total_loss(j_style , j_content, style_weight, content_weight)\n\n      norm_means = np.array([103.939, 116.779, 123.68])\n      min_vals = -norm_means\n      max_vals = 255 - norm_means\n      grad = g.gradient(j_total , gen_image)\n      optimizer.apply_gradients([(grad, gen_image)])\n      gen_image.assign(clip(gen_image, min_vals, max_vals))\n      return j_total, j_style, j_content\n\n  def __call__(self, model, style_path, content_path, learning_rate, style_weight, content_weight, epochs):\n\n      history = {\"content_loss\":[], \"style_loss\":[], \"total_loss\":[]}\n      style_image = img_preprocess(style_path)\n      content_image = img_preprocess(content_path)\n\n      # getting content and style features\n      style_features = get_features(model, style_image)[\"style\"]\n      content_features = get_features(model, content_image)[\"content\"]\n\n      # adding some noise to the content_image\n      content_img_shape = tf.shape(content_image)\n      noise = tf.random.uniform(content_img_shape, minval=0, maxval=0.5)\n      gen_image = tf.add(content_image, noise)\n      gen_image = tf.Variable(gen_image, tf.float32)\n      # changing the gen_image\n      opt = tf.keras.optimizers.Adam(learning_rate, beta_1=0.99, epsilon=1e-8)\n      for i in range(epochs):\n          all_losses = self.gradient_descent(model,\n                                        opt,\n                                        gen_image,\n                                        style_features,\n                                        content_features,\n                                        style_weight,\n                                        content_weight)\n\n          history[\"total_loss\"].append(all_losses[0].numpy())\n          history[\"style_loss\"].append(all_losses[1].numpy())\n          history[\"content_loss\"].append(all_losses[2].numpy())\n\n          if i % 100 == 0:\n              img = PIL.Image.fromarray(deprocess_img(gen_image.numpy()))\n              display.clear_output(wait=True)\n              display.display_png(img)\n              print(f\"epoch: {i}\\ntotal_loss: {all_losses[0]}\")\n\n      return history\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:53:16.194124Z","iopub.execute_input":"2023-09-01T12:53:16.196114Z","iopub.status.idle":"2023-09-01T12:53:16.210687Z","shell.execute_reply.started":"2023-09-01T12:53:16.196088Z","shell.execute_reply":"2023-09-01T12:53:16.209660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arguments_dict = { \"model\" : transfer_model,\n        \"style_path\" : \"Vassily_Kandinsky,_1913_-_Composition_7.jpg\",\n        \"content_path\" : \"YellowLabradorLooking_new.jpg\",\n        \"learning_rate\" : 5,\n        \"style_weight\" : 10,\n        \"content_weight\" : 1e3,\n        \"epochs\" : 3001}\n\ntransfer1 = StyleTransfer()\nhistory = transfer1(**arguments_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:53:16.213901Z","iopub.execute_input":"2023-09-01T12:53:16.214223Z","iopub.status.idle":"2023-09-01T12:57:28.169557Z","shell.execute_reply.started":"2023-09-01T12:53:16.214162Z","shell.execute_reply":"2023-09-01T12:57:28.168723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,3))\n\nplt.subplot(1,3,1)\nplt.plot(history[\"content_loss\"])\nplt.title(\"Content loss\")\n\nplt.subplot(1,3,2)\nplt.plot(history[\"style_loss\"])\nplt.title(\"Style loss\")\n\nplt.subplot(1,3,3)\nplt.plot(history[\"total_loss\"])\nplt.title(\"Total loss\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:57:28.170914Z","iopub.execute_input":"2023-09-01T12:57:28.171840Z","iopub.status.idle":"2023-09-01T12:57:28.714007Z","shell.execute_reply.started":"2023-09-01T12:57:28.171803Z","shell.execute_reply":"2023-09-01T12:57:28.712979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example 2 ","metadata":{}},{"cell_type":"code","source":"!wget https://www.planetegrandesecoles.com/wp-content/uploads/2023/02/estimation-nuit-etoilee-tableau-van-gogh-850x560.jpg\n!wget https://nt.global.ssl.fastly.net/binaries/content/gallery/website/national/regions/wales/places/powis-castle-and-garden/library/castle-terraces-view-powis-castle-wales-1047435.jpg","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:57:28.715422Z","iopub.execute_input":"2023-09-01T12:57:28.716460Z","iopub.status.idle":"2023-09-01T12:57:32.178191Z","shell.execute_reply.started":"2023-09-01T12:57:28.716424Z","shell.execute_reply":"2023-09-01T12:57:32.177033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,5))\n\ncontent_image2 = load_image(\"castle-terraces-view-powis-castle-wales-1047435.jpg\")\nstyle_image2 = load_image(\"estimation-nuit-etoilee-tableau-van-gogh-850x560.jpg\")\n\n\nplt.subplot(121)\nshow(content_image2[0], \"Content image\")\n\nplt.subplot(122)\nshow(style_image2[0] , \"Style image\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:57:32.180497Z","iopub.execute_input":"2023-09-01T12:57:32.180915Z","iopub.status.idle":"2023-09-01T12:57:32.596907Z","shell.execute_reply.started":"2023-09-01T12:57:32.180873Z","shell.execute_reply":"2023-09-01T12:57:32.596091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arguments_dict = { \"model\" : transfer_model,\n        \"style_path\" : \"estimation-nuit-etoilee-tableau-van-gogh-850x560.jpg\",\n        \"content_path\" : \"castle-terraces-view-powis-castle-wales-1047435.jpg\",\n        \"learning_rate\" : 5,\n        \"style_weight\" : 1e-2,\n        \"content_weight\" : 1e4,\n        \"epochs\" : 3001}\n\ntransfer2 = StyleTransfer()\nhistory = transfer2(**arguments_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T12:57:32.598109Z","iopub.execute_input":"2023-09-01T12:57:32.598713Z","iopub.status.idle":"2023-09-01T13:00:30.386522Z","shell.execute_reply.started":"2023-09-01T12:57:32.598678Z","shell.execute_reply":"2023-09-01T13:00:30.385549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example 3","metadata":{}},{"cell_type":"code","source":"!wget https://www.artisangallery.fr/4411/tableau-roses-pansies-and-other-flowers-in-a-vase-oil-on-canvas-.jpg\n!wget https://www.parcanimalierlabarben.com/wp-content/uploads/2014/10/lynx-07082020-1.jpg    ","metadata":{"execution":{"iopub.status.busy":"2023-09-01T13:00:30.387935Z","iopub.execute_input":"2023-09-01T13:00:30.388510Z","iopub.status.idle":"2023-09-01T13:00:36.697211Z","shell.execute_reply.started":"2023-09-01T13:00:30.388475Z","shell.execute_reply":"2023-09-01T13:00:36.695955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_image3 = load_image(\"lynx-07082020-1.jpg\")\nstyle_image3 = load_image(\"tableau-roses-pansies-and-other-flowers-in-a-vase-oil-on-canvas-.jpg\")\n\nplt.figure(figsize = (10,5))\nplt.subplot(121)\nshow(content_image3[0], \"Content image\")\n\nplt.subplot(122)\nshow(style_image3[0] , \"Style image\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T13:00:36.701942Z","iopub.execute_input":"2023-09-01T13:00:36.702394Z","iopub.status.idle":"2023-09-01T13:00:37.461519Z","shell.execute_reply.started":"2023-09-01T13:00:36.702355Z","shell.execute_reply":"2023-09-01T13:00:37.459851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arguments_dict = { \"model\" : transfer_model,\n        \"style_path\" : \"tableau-roses-pansies-and-other-flowers-in-a-vase-oil-on-canvas-.jpg\",\n        \"content_path\" : \"lynx-07082020-1.jpg\",\n        \"learning_rate\": 10,\n        \"style_weight\" : 1e-1,\n        \"content_weight\" : 1e4,\n        \"epochs\" : 1001}\n\ntransfer3 = StyleTransfer()\nhistory = transfer3(**arguments_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T13:00:37.463098Z","iopub.execute_input":"2023-09-01T13:00:37.463738Z","iopub.status.idle":"2023-09-01T13:02:19.887391Z","shell.execute_reply.started":"2023-09-01T13:00:37.463705Z","shell.execute_reply":"2023-09-01T13:02:19.886554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example 4","metadata":{}},{"cell_type":"code","source":"!wget https://cdn.topofart.com/images/artists/Edvard-Munch/paintings-wm/munch004.jpg\n!wget https://thedailyapologist.com/images/featured/_1553x874_crop_center-center_80_none/AdobeStock_317104103.jpeg","metadata":{"execution":{"iopub.status.busy":"2023-09-01T13:09:07.616227Z","iopub.execute_input":"2023-09-01T13:09:07.616662Z","iopub.status.idle":"2023-09-01T13:09:10.188409Z","shell.execute_reply.started":"2023-09-01T13:09:07.616625Z","shell.execute_reply":"2023-09-01T13:09:10.187249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_image3 = load_image(\"AdobeStock_317104103.jpeg\")\nstyle_image3 = load_image(\"munch004.jpg\")\n\nplt.figure(figsize = (10,5))\nplt.subplot(121)\nshow(content_image3[0], \"Content image\")\n\nplt.subplot(122)\nshow(style_image3[0] , \"Style image\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T13:09:26.207003Z","iopub.execute_input":"2023-09-01T13:09:26.207405Z","iopub.status.idle":"2023-09-01T13:09:26.600848Z","shell.execute_reply.started":"2023-09-01T13:09:26.207369Z","shell.execute_reply":"2023-09-01T13:09:26.599904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arguments_dict = { \"model\" : transfer_model,\n        \"style_path\" : \"munch004.jpg\",\n        \"content_path\" : \"AdobeStock_317104103.jpeg\",\n        \"learning_rate\": 5,\n        \"style_weight\" : 1e-1,\n        \"content_weight\" : 1e4,\n        \"epochs\" : 1001}\n\ntransfer4 = StyleTransfer()\nhistory = transfer4(**arguments_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T13:09:41.405869Z","iopub.execute_input":"2023-09-01T13:09:41.406509Z","iopub.status.idle":"2023-09-01T13:10:39.130744Z","shell.execute_reply.started":"2023-09-01T13:09:41.406470Z","shell.execute_reply":"2023-09-01T13:10:39.127539Z"},"trusted":true},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}